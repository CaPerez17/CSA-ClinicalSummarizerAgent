# Clinical Summarizer Agent

Microservicio completo para resumir conversaciones clÃ­nicas usando LLM y arquitectura de cola.

## ğŸ¯ Objetivo del Proyecto

Este proyecto demuestra cÃ³mo construir un microservicio escalable que procesa datos clÃ­nicos usando modelos de IA, siguiendo mejores prÃ¡cticas de arquitectura de software.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cliente â”‚â”€â”€â”€â”€â”€â–¶â”‚   API   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Redis  â”‚â—€â”€â”€â”€â”€â”€â”‚ Worker  â”‚
â”‚         â”‚      â”‚ FastAPI â”‚      â”‚  Queue  â”‚      â”‚(Inference)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                  â”‚                â”‚
                      â”‚                  â”‚                â”‚
                      â–¼                  â–¼                â–¼
                 /submit          Encola trabajo    Procesa trabajo
                 /result/{id}     Almacena resultado
```

### Componentes Principales

1. **API (FastAPI)**: Recibe peticiones HTTP, encola trabajos, devuelve resultados
2. **Redis**: Cola de trabajos y almacenamiento temporal de resultados
3. **Worker**: Proceso separado que ejecuta inference (Whisper + LLM)

## ğŸ“š Conceptos Clave Explicados

### 1. Â¿QuÃ© es Inference?

**Inference** es el proceso de ejecutar un modelo de IA entrenado con datos nuevos para obtener una predicciÃ³n o salida.

En este proyecto:
- **Whisper**: Toma audio â†’ genera texto (inference de transcripciÃ³n)
- **LLM (GPT-4)**: Toma texto â†’ genera resumen estructurado (inference de lenguaje)

**Â¿Por quÃ© es lento?**
- Los modelos tienen millones/billones de parÃ¡metros
- Cada predicciÃ³n requiere cÃ¡lculos matemÃ¡ticos complejos
- Puede tardar segundos o minutos dependiendo del tamaÃ±o del input

### 2. Â¿Por quÃ© NO ejecutar Inference en la API?

**Problema si ejecutamos inference en la ruta de la API:**

```python
# âŒ MAL - Bloquea el servidor
@app.post("/submit")
async def submit():
    result = model.predict(data)  # Tarda 30 segundos
    return result  # Cliente espera 30 segundos
```

**Problemas:**
- Cliente espera bloqueado (mala UX)
- Servidor no puede atender otras peticiones eficientemente
- Si hay muchos clientes, el servidor se satura
- No escala horizontalmente

**SoluciÃ³n con cola:**

```python
# âœ… BIEN - Responde inmediatamente
@app.post("/submit")
async def submit():
    job_id = enqueue_job(data)  # Tarda 10ms
    return {"job_id": job_id}  # Cliente recibe respuesta inmediata
```

**Ventajas:**
- API responde en milisegundos
- Worker procesa en background
- Puedes tener mÃºltiples workers procesando en paralelo
- Escala horizontalmente aÃ±adiendo mÃ¡s workers

### 3. Arquitectura con Cola (Queue Architecture)

**Flujo completo:**

1. **Cliente â†’ API (`/submit`)**:
   - Cliente envÃ­a texto o audio
   - API valida datos
   - API encola trabajo en Redis
   - API devuelve `job_id` inmediatamente

2. **Worker escucha cola**:
   - Worker estÃ¡ ejecutÃ¡ndose en un proceso separado
   - Escucha la cola de Redis constantemente
   - Cuando encuentra un trabajo, lo toma

3. **Worker procesa trabajo**:
   - Si hay audio, transcribe con Whisper
   - Procesa texto con LLM
   - Guarda resultado en Redis

4. **Cliente â†’ API (`/result/{job_id}`)**:
   - Cliente hace polling al endpoint
   - API consulta Redis
   - Devuelve resultado cuando estÃ¡ listo

**Desacoplamiento:**
- API y Worker estÃ¡n completamente desacoplados
- Pueden ejecutarse en servidores diferentes
- Puedes escalar cada uno independientemente

### 4. FastAPI y Concurrencia

**FastAPI usa ASGI (Asynchronous Server Gateway Interface):**

```python
async def endpoint():
    # async permite que FastAPI maneje mÃºltiples peticiones
    # sin bloquear el servidor
    result = await some_async_operation()
    return result
```

**Â¿CÃ³mo funciona?**
- `async/await` permite que Python pause una funciÃ³n y ejecute otra
- Cuando una funciÃ³n espera I/O (red, disco), Python puede ejecutar otra funciÃ³n
- Esto permite manejar miles de conexiones concurrentes

**LimitaciÃ³n importante:**
- `async/await` es excelente para I/O (red, base de datos)
- **NO** paraleliza operaciones CPU-intensivas (como inference)
- Por eso necesitamos workers separados para inference

### 5. CÃ³mo Construir un Agente ClÃ­nico

Un agente clÃ­nico es un sistema que:
1. **Entiende** lenguaje mÃ©dico natural
2. **Extrae** informaciÃ³n estructurada
3. **Estructura** datos en formato estÃ¡ndar

**Nuestro agente:**

```python
class ClinicalAgent:
    def process_clinical_text(self, text: str):
        # 1. Construir prompt detallado
        prompt = self._build_clinical_prompt(text)
        
        # 2. Enviar a LLM (inference)
        response = llm.generate(prompt)
        
        # 3. Parsear respuesta
        structured_data = self._parse_response(response)
        
        # 4. Validar y estructurar
        return ClinicalSummary(**structured_data)
```

**Prompt Engineering:**
- El prompt le dice al LLM exactamente quÃ© hacer
- Incluye ejemplos y formato esperado
- Es crÃ­tico para obtener buenos resultados

### 6. ConversiÃ³n a FHIR

**FHIR (Fast Healthcare Interoperability Resources)** es un estÃ¡ndar para intercambio de informaciÃ³n mÃ©dica.

**Â¿Por quÃ© es importante?**
- Permite que diferentes sistemas mÃ©dicos se comuniquen
- Estructura datos de manera consistente
- Facilita integraciÃ³n con sistemas hospitalarios

**Nuestro mÃ³dulo FHIR:**
- Convierte `ClinicalSummary` â†’ formato FHIR Bundle
- Crea recursos: Patient, Condition, Observation, ClinicalImpression
- Mantiene compatibilidad con sistemas FHIR

### 7. DiseÃ±o de Microservicios Escalables

**Principios aplicados:**

1. **SeparaciÃ³n de responsabilidades**:
   - API: Maneja HTTP, valida datos
   - Worker: Ejecuta inference
   - Redis: Cola y almacenamiento

2. **Escalabilidad horizontal**:
   - Puedes ejecutar mÃºltiples instancias del API
   - Puedes ejecutar mÃºltiples workers
   - Redis maneja la distribuciÃ³n

3. **Resiliencia**:
   - Si un worker falla, otro puede tomar el trabajo
   - Si el API se cae, los trabajos siguen en la cola
   - Redis persiste datos en disco

4. **Monitoreo**:
   - Health checks (`/health`)
   - Logging estructurado
   - Estados de trabajos rastreables

### 8. WebSockets en Workflows de Salud

**Â¿Por quÃ© WebSockets?**
- Permiten comunicaciÃ³n bidireccional en tiempo real
- Cliente puede recibir actualizaciones sin hacer polling
- Mejor UX para procesos largos

**Ejemplo de uso:**
```python
@app.websocket("/ws/{job_id}")
async def websocket_endpoint(websocket: WebSocket, job_id: str):
    # Cliente se conecta
    await websocket.accept()
    
    # Enviar actualizaciones en tiempo real
    while True:
        status = get_job_status(job_id)
        await websocket.send_json(status)
        
        if status == "completed":
            break
```

**Ventajas:**
- Cliente recibe actualizaciones automÃ¡ticamente
- Menos carga en el servidor (no hay polling constante)
- Mejor experiencia de usuario

## ğŸš€ InstalaciÃ³n y Uso

### Prerrequisitos

- Python 3.11+
- Redis (o Docker)
- OpenAI API Key (para el LLM)

### InstalaciÃ³n Local

1. **Clonar y entrar al directorio:**
```bash
cd CSA-ClinicalSummarizerAgent
```

2. **Crear entorno virtual:**
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

4. **Configurar variables de entorno:**
```bash
cp .env.example .env
# Editar .env y agregar tu OPENAI_API_KEY
```

5. **Iniciar Redis:**
```bash
# OpciÃ³n 1: Docker
docker run -d -p 6379:6379 redis:7-alpine

# OpciÃ³n 2: InstalaciÃ³n local
redis-server
```

6. **Iniciar API (en una terminal):**
```bash
uvicorn app.main:app --reload
```

7. **Iniciar Worker (en otra terminal):**
```bash
rq worker clinical_jobs
```

### Uso con Docker Compose

```bash
# Construir e iniciar todos los servicios
docker-compose up --build

# Ver logs
docker-compose logs -f

# Detener servicios
docker-compose down
```

## ğŸ“– Uso de la API

### 1. Enviar trabajo con texto

```bash
curl -X POST "http://localhost:8000/submit" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Paciente de 45 aÃ±os, masculino, presenta dolor de cabeza desde hace 3 dÃ­as, severidad moderada. Tiene historial de migraÃ±as."
  }'
```

**Respuesta:**
```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "pending",
  "message": "Trabajo encolado exitosamente..."
}
```

### 2. Consultar resultado

```bash
curl "http://localhost:8000/result/123e4567-e89b-12d3-a456-426614174000"
```

**Respuesta (cuando estÃ¡ completado):**
```json
{
  "job_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "clinical_summary": {
    "patient_age": 45,
    "patient_gender": "masculino",
    "symptoms": [
      {
        "name": "dolor de cabeza",
        "duration": "3 dÃ­as",
        "severity": "moderado"
      }
    ],
    "risk_factors": ["historial de migraÃ±as"],
    "relevant_conditions": ["migraÃ±a"],
    "narrative_summary": "Paciente masculino de 45 aÃ±os..."
  }
}
```

### 3. Health Check

```bash
curl "http://localhost:8000/health"
```

## ğŸ§ª Testing

```bash
# Ejecutar tests (cuando estÃ©n implementados)
pytest tests/
```

## ğŸ“ Estructura del Proyecto

```
CSA-ClinicalSummarizerAgent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py          # Paquete Python
â”‚   â”œâ”€â”€ main.py              # FastAPI app y endpoints
â”‚   â”œâ”€â”€ worker.py            # Worker que ejecuta inference
â”‚   â”œâ”€â”€ agent.py             # Agente clÃ­nico (LLM)
â”‚   â”œâ”€â”€ fhir.py              # ConversiÃ³n a formato FHIR
â”‚   â”œâ”€â”€ models.py            # Schemas Pydantic
â”‚   â”œâ”€â”€ queue.py             # Manejo de Redis
â”‚   â””â”€â”€ config.py            # ConfiguraciÃ³n
â”œâ”€â”€ tests/                   # Tests unitarios e integraciÃ³n
â”œâ”€â”€ docker-compose.yml       # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile               # Imagen Docker
â”œâ”€â”€ requirements.txt         # Dependencias Python
â””â”€â”€ README.md               # Este archivo
```

## ğŸ” Conceptos para la Entrevista

### Preguntas que puedes responder ahora:

1. **Â¿QuÃ© es inference y por quÃ© es lento?**
   - Inference es ejecutar un modelo con datos nuevos
   - Es lento porque requiere cÃ¡lculos complejos en millones de parÃ¡metros

2. **Â¿Por quÃ© no ejecutar inference en la API?**
   - Bloquea el servidor, mala UX, no escala
   - SoluciÃ³n: cola + workers separados

3. **Â¿CÃ³mo funciona una arquitectura con cola?**
   - API encola trabajos rÃ¡pidamente
   - Workers procesan en background
   - Resultados se almacenan en Redis

4. **Â¿CÃ³mo escala este sistema?**
   - MÃºltiples instancias del API (load balancer)
   - MÃºltiples workers procesando en paralelo
   - Redis distribuye trabajos

5. **Â¿QuÃ© es FHIR y por quÃ© es importante?**
   - EstÃ¡ndar para intercambio de informaciÃ³n mÃ©dica
   - Permite interoperabilidad entre sistemas

## ğŸ“ PrÃ³ximos Pasos para Aprender

1. **Implementar WebSocket endpoint** para streaming
2. **AÃ±adir tests unitarios** para cada mÃ³dulo
3. **Implementar retry logic** en el worker
4. **AÃ±adir mÃ©tricas y monitoreo** (Prometheus)
5. **Implementar autenticaciÃ³n** (JWT tokens)
6. **AÃ±adir rate limiting** para prevenir abuso
7. **Optimizar prompts** del LLM para mejor precisiÃ³n
8. **Implementar caching** para resultados similares

## ğŸ“ Notas Importantes

- Este es un proyecto educativo/demostraciÃ³n
- Para producciÃ³n, necesitarÃ­as:
  - AutenticaciÃ³n y autorizaciÃ³n
  - Manejo robusto de errores
  - Logging y monitoreo completo
  - Tests exhaustivos
  - DocumentaciÃ³n API completa
  - CI/CD pipeline
  - Manejo de datos sensibles (HIPAA compliance)

## ğŸ¤ Contribuciones

Este proyecto es parte de la preparaciÃ³n para entrevistas tÃ©cnicas.
SiÃ©ntete libre de mejorarlo y experimentar con diferentes enfoques.

## ğŸ“„ Licencia

Este proyecto es educativo y estÃ¡ disponible para uso personal.

